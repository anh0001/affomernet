task: detection

model: RTDETR
criterion: SetCriterion
postprocessor: RTDETRPostProcessor


RTDETR: 
  backbone: PResNet
  encoder: HybridEncoder
  decoder: RTDETRTransformer
  affordance_branch: AffordanceBranch
  multi_scale: [480, 512, 544, 576, 608, 640, 640, 640, 672, 704, 736, 768, 800]

PResNet:
  depth: 50
  variant: d
  freeze_at: 0
  return_idx: [1, 2, 3]
  num_stages: 4
  freeze_norm: True
  pretrained: True 

HybridEncoder:
  in_channels: [512, 1024, 2048]
  feat_strides: [8, 16, 32]

  # intra
  hidden_dim: 256
  use_encoder_idx: [2]
  num_encoder_layers: 1
  nhead: 8
  dim_feedforward: 1024
  dropout: 0.
  enc_act: 'gelu'
  pe_temperature: 10000
  
  # cross
  expansion: 1.0
  depth_mult: 1
  act: 'silu'

  # eval
  eval_spatial_size: [640, 640]


RTDETRTransformer:
  feat_channels: [256, 256, 256]
  feat_strides: [8, 16, 32]
  hidden_dim: 256
  num_levels: 3

  num_queries: 300

  num_decoder_layers: 6
  num_denoising: 100
  
  eval_idx: -1
  eval_spatial_size: [640, 640]

# The 'features' returned by the TransformerDecoder will have the shape [num_layers, batch_size, num_queries, hidden_dim].
# In the RTDETR class, so decoder_output['features'][-1], which gives you the features from the last decoder layer, with shape [batch_size, num_queries, hidden_dim].
AffordanceBranch:
  input_dim: 256  # Should match the output dimension of the RTDETRTransformer. This should match the hidden_dim of RTDETRTransformer
  hidden_dim: 512
  output_dim: 10  # 9 affordance classes + 1 background
  num_conv_layers: 6
  num_deconv_layers: 3

use_focal_loss: True

RTDETRPostProcessor:
  num_top_queries: 300


SetCriterion:
  weight_dict: {loss_vfl: 1, loss_bbox: 5, loss_giou: 2, loss_affordance: 3}
  losses: ['vfl', 'boxes', 'affordance']
  alpha: 0.75
  gamma: 2.0

  matcher:
    type: HungarianMatcher
    weight_dict: {cost_class: 2, cost_bbox: 5, cost_giou: 2}
    # use_focal_loss: True 
    alpha: 0.25
    gamma: 2.0



